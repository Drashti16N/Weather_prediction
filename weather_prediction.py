# -*- coding: utf-8 -*-
"""weather_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YzRZEzVY6E6uTR-qr7LB-2GK0xsv-QtG
"""

# Importing different libraries for further use:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Reading data(CSV file) and display 5 rows of data:
data = pd.read_csv("/content/Final_Dehradun_weather.csv")
data.head()

"""Data view:"""

# View total number of rows and columns in data:
print(f"Number of Rows: {data.shape[0]}\nNumber of Columns: {data.shape[1]}")

# View features(columns) of data:
print(f"List of Columns: {data.columns}")

# Display datatype of individual columns of data:
print(f"Data types of columns:\n{data.dtypes}")

# Checking for Nan or null values in data:
data.info()

# Display total counts of labeled weather conditions:
data.Weather_condition.value_counts()

"""Extracting samples from the data:"""

# Extracting random 50 samples of "Moderate cold" temperature:
Moderate_cold_df = data[data["Weather_condition"] == "Moderate cold"]
Moderate_cold_df_sample = Moderate_cold_df.sample(50)
Moderate_cold_df_sample.shape

# Extracting random 40 samples of "Rainy" temperature:
Rainy_df = data[data["Weather_condition"] == "Rainy"]
Rainy_df_sample = Rainy_df.sample(40)
Rainy_df_sample.shape

# Extracting random 20 samples of "cloudy" temperature:
Cloudy_df = data[data["Weather_condition"] == "Cloudy"]
Cloudy_df_sample = Cloudy_df.sample(20)
Cloudy_df_sample.shape

# Extracting random 25 samples of "Heavy rain" temperature:
Heavy_rain_df = data[data["Weather_condition"] == "Heavy rain"]
Heavy_rain_df_sample = Heavy_rain_df.sample(25)
Heavy_rain_df_sample.shape

# Extracting random 15 samples of "Clear" temperature:
Clear_df = data[data["Weather_condition"] == "Clear"]
Clear_df_sample = Clear_df.sample(15)
Clear_df_sample.shape

# Extracting random 20 samples of "Cold and rainy" temperature:
Cold_and_rainy_df = data[data["Weather_condition"] == "Cold and Rainy"]
Cold_and_rainy_df_sample = Cold_and_rainy_df.sample(20)
Cold_and_rainy_df_sample.shape

# Extracting random 8 samples of "cold" temperature:
Cold_df = data[data["Weather_condition"] == "Cold"]
Cold_df_sample = Cold_df.sample(8)
Cold_df_sample.shape

# Extracting random 5 samples of "Extreme cold" temperature:
Extreme_cold_df = data[data["Weather_condition"] == "Extreme cold"]
Extreme_cold_df_sample = Extreme_cold_df.sample(5)
Extreme_cold_df_sample.shape

"""Create new weather dataset:"""

# create new data set with only samples of different weather conditions:
weather_df = pd.concat([Moderate_cold_df_sample,Rainy_df_sample,Cloudy_df_sample,Heavy_rain_df_sample,Clear_df_sample,Cold_and_rainy_df_sample,Cold_df_sample,Extreme_cold_df_sample])
# View 5 rows of the new data set:
weather_df.head()

# Displaying Number of rows and columns in new dataset:
weather_df.shape
print(f"Number of rows in new dataset= {weather_df.shape[0]}")
print(f"Number of columns in new dataset= {weather_df.shape[1]}")

# Counting samples of each weather conditions:
weather_df.Weather_condition.value_counts()

"""Drop Columns:"""

# Droping unrelavient columns like district, year, month and day:
weather_df.drop(columns = ['District','Year','Month','Day'],axis = 1, inplace = True)

# Check dataset after droping columns:
weather_df.head()

"""Duplicate Records:"""

# Checking for duplicate columns:
weather_df[weather_df.duplicated()]

"""Null/ Missing values:"""

# Check null values in dataset:
weather_df.isnull().sum()

# Display datatypes of all the columns in dataset:
weather_df.dtypes

"""Data Visulization:"""

# View statistics of data like mean, standard deviation, minimum, maximum etc.:
weather_df.describe()

"""Correlation among the features:"""

cols = ['Temperature_C','Relative_humidity_%', 'Precipitation_mm/hour', 'Wind_speed_m/s']

# Create correlation matrix to check correlation among different features of dataset:
cor_matrix = weather_df[cols].corr()
cor_matrix

"""Heat Map:"""

# Ploting heatmap of correlation matrix for better visualization:
sns.heatmap(cor_matrix, annot = True)

weather_df.columns

# Histogram of Temperature column:
weather_df['Temperature_C'].plot(kind="hist", color= 'b')

# Histogram of Relative humidity column:
weather_df['Relative_humidity_%'].plot(kind='hist', color= 'y')

# Histogram of Wind speed column:
weather_df['Wind_speed_m/s'].plot(kind="hist", color= 'g')

# Histogram of Precipitation column:
weather_df['Precipitation_mm/hour'].plot(kind= "hist", color= 'r')

# Create boxplot of different columns for checking distribution and outliers in dataset:
# Boxplot of Temperature column:
weather_df['Temperature_C'].plot(kind="box")

# Boxplot of Relative humidity column:
weather_df['Relative_humidity_%'].plot(kind="box", color= 'g')

# Boxplot of Precipitation column:
weather_df['Precipitation_mm/hour'].plot(kind="box")

# Boxplot of Wind speed column:
weather_df['Wind_speed_m/s'].plot(kind="box")

"""**Label Encoding:**
Output column, "weather conditions" have various types of weather. This categorical column must be converted into numeric to use it in different classification algorithm.
"""

# Importing Label encoder for assigning unique value to each category in output column from sklearn preprocessing:
from sklearn.preprocessing import LabelEncoder

# Make object of label encoder:
label_Encoder = LabelEncoder()

# Fit the column to encode:
weather_df['Weather_condition'] = label_Encoder.fit_transform(weather_df['Weather_condition'])

# View number of categories to encode:
label_Encoder.classes_

# Check dataset after label encoding:
weather_df.head()

weather_df.Weather_condition.value_counts()

# Independent Variables:
x = weather_df.drop(['Weather_condition'], axis = 1)
x

# Target Variable:
y = weather_df['Weather_condition']
y

"""**Feature Scalling:**
Brings all the data into a common scale.
"""

# Importing standard scaler form sklearn preprocessing:
from sklearn.preprocessing import StandardScaler

# Make object of standard scaler:
std_scaler = StandardScaler()

# fit input data to standard scaler object:
x_std = std_scaler.fit_transform(x)

# View transformed data:
x_std[0:6]

"""Splitting Data into training and testing:"""

# Importing train test split from sklearn preprocessing:
from sklearn.model_selection import train_test_split

# Spliting entire dataset into 80% train and 20% test data:
x_train, x_test, y_train, y_test = train_test_split(x_std, y, test_size = 0.2)

# Display row and columns of train and test data:
x_train.shape, x_test.shape
print("-------TRAIN DATA-------")
print(f"Number of rows in train data= {x_train.shape[0]}")
print(f"Number of columns in train data= {x_train.shape[1]}")
print("-------TEST DATA-------")
print(f"Number of rows in test data= {x_test.shape[0]}")
print(f"Number of columns in test data= {x_test.shape[1]}")

"""Model Training:"""

# Importing Decision tree classifier from sklearn:
from sklearn.tree import DecisionTreeClassifier
decision_tree_model = DecisionTreeClassifier()

# Model Training:
decision_tree_model.fit(x_train, y_train)

# Model Predictions:
y_pred_dt = decision_tree_model.predict(x_test)

"""Model Evaluation:"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Accuracy assisment:
accuracy_score(y_test, y_pred_dt)

print(classification_report(y_test, y_pred_dt))

con_matrix = confusion_matrix(y_test, y_pred_dt)
sns.heatmap(con_matrix, annot = True, fmt = 'd')

#Importing Random forest classifier from sklearn:
from sklearn.ensemble import RandomForestClassifier

# Create object of random forest classifier:
rf_model = RandomForestClassifier()

# Training model:
rf_model.fit(x_train, y_train)

# Prediction:
y_pred_rf = rf_model.predict(x_test)

# Accuracy assisment:
accuracy_score(y_test, y_pred_rf)

# Training various models like svm, knn, naive bayes etc, for classification:
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

# creating objects of different models:
svc_model = SVC()
knn_model = KNeighborsClassifier()
lr_model = LogisticRegression()
nb_model = GaussianNB()

# List of all the models:
model_list = [decision_tree_model, rf_model, svc_model, knn_model, lr_model, nb_model]

# Creating list of accuracy of all the models and selecting best model with highest accuracy:
accuracy_list = []
for model in model_list:
  model.fit(x_train, y_train)
  y_pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, y_pred)
  accuracy_list.append(accuracy)

accuracy_list

model_list_1 = ["Decision Tree","Random Forest","SVC","KNN","Logistic Regression","Gaussian"]

# Creating dataframe of model with their respective accuracy:
model_df = pd.DataFrame({"Model": model_list_1, "Accuracy": accuracy_list})
model_df

"""k-fold cross validation:"""

# Importing cross validation score from model selection of sklearn:
from sklearn.model_selection import cross_val_score

# Performing cross validation of Decision tree classifier:
scores_dt = cross_val_score(decision_tree_model, x_std, y, cv = 4, scoring = "accuracy")
print(f"Cross-validation of decision tree score= {scores_dt}")
print(f"Mean of score= {scores_dt.mean()}")

# Performing cross validation of radom forest classifier:
from sklearn.model_selection import cross_val_score
scores_rf = cross_val_score(rf_model, x_std, y, cv = 4, scoring = "accuracy")
print(f"Cross-validation of random forest score= {scores_rf}")
print(f"Mean of score= {scores_rf.mean()}")

"""Hyperparameter tunning: Model tunning"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Hyperparameters of Decision tree classifier:
params = {
    'max_depth': [1,2,3,4,5,None]
}

grid_search_dt = GridSearchCV(decision_tree_model, params)
grid_search_dt.fit(x_train, y_train)

# Hyperparameters of random forest classifier:
parameters = {
    'n_estimators': [50,100],
    'max_features': ['sqrt','log2', None]
}

grid_search =GridSearchCV(estimator = rf_model,
                          param_grid = parameters)
grid_search.fit(x_train, y_train)

"""Best Hyper Parameter:"""

# Searching best parameter for decision tree classifier:
grid_search_dt.best_params_

# Create new decision tree model:
new_decision_tree_model = DecisionTreeClassifier(max_depth=5)
new_decision_tree_model.fit(x_train, y_train)

# Cross validaton score after hyper parameter tuning of decision tree classifier:
new_scores_dt = cross_val_score(new_decision_tree_model, x_std, y, cv=4, scoring="accuracy")
print(f"new cross validation score of decision tree classifier= {new_scores_dt}")
print(f"new mean score of decision tree classifier= {new_scores_dt.mean()}")

# Searching best parameter for random forest classifier:
grid_search.best_params_

# Create new random forest model:
new_random_forest_model = RandomForestClassifier(max_features= "sqrt", n_estimators = 100)
new_random_forest_model.fit(x_train, y_train)

# Cross validaton score after hyper parameter tuning of random forest classifier:
new_scores_rf = cross_val_score(new_random_forest_model, x_std, y, cv = 4, scoring = "accuracy")
print(f"new cross validation score of Random forest classifier= {new_scores_rf}")
print(f"new mean score Random forest classifier= {new_scores_rf.mean()}")

model_df.plot.bar(x="Model", y="Accuracy")